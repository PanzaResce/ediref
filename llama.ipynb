{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, LlamaModel, LlamaForCausalLM, AutoConfig, AutoModel\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "\n",
    "from src.utils import *\n",
    "from datasets import Dataset\n",
    "from src.models.baseline import Baseline\n",
    "from torch.nn.functional import one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: d:\\shared\\unibo\\year2\\NLP\\assignments\\project\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "      <th>emotions_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>utterance_0</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise]</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>utterance_1</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utterance_2</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>utterance_3</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utterance_4</td>\n",
       "      <td>[surprise, sadness, surprise, fear]</td>\n",
       "      <td>[But then who? The waitress I went out with la...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>utterance_3995</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>utterance_3996</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>utterance_3997</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>utterance_3998</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>utterance_3999</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             episode                                           emotions  \\\n",
       "0        utterance_0     [neutral, neutral, neutral, neutral, surprise]   \n",
       "1        utterance_1  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "2        utterance_2  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "3        utterance_3  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "4        utterance_4                [surprise, sadness, surprise, fear]   \n",
       "...              ...                                                ...   \n",
       "3995  utterance_3995  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3996  utterance_3996  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3997  utterance_3997  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3998  utterance_3998  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3999  utterance_3999  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "\n",
       "                                             utterances  \\\n",
       "0     [also I was the point person on my company's t...   \n",
       "1     [also I was the point person on my company's t...   \n",
       "2     [also I was the point person on my company's t...   \n",
       "3     [also I was the point person on my company's t...   \n",
       "4     [But then who? The waitress I went out with la...   \n",
       "...                                                 ...   \n",
       "3995  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3996  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3997  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3998  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3999  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "\n",
       "                                               triggers  \\\n",
       "0                                       [0, 0, 0, 1, 0]   \n",
       "1                                 [0, 0, 0, 0, 0, 1, 0]   \n",
       "2                     [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]   \n",
       "3               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "4                                          [0, 0, 1, 0]   \n",
       "...                                                 ...   \n",
       "3995               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "3996         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "3997      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "3998   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "3999  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            emotions_id  \n",
       "0     [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....  \n",
       "1     [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....  \n",
       "2     [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....  \n",
       "3     [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....  \n",
       "4     [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0....  \n",
       "...                                                 ...  \n",
       "3995  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....  \n",
       "3996  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....  \n",
       "3997  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....  \n",
       "3998  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....  \n",
       "3999  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....  \n",
       "\n",
       "[4000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED=42\n",
    "\n",
    "url = \"https://drive.google.com/uc?export=download&id=1wVNU2XvvhqjaGXZM-JLJwOt97gt4g9j2\"\n",
    "dataset_name = \"MELD_train_efr.json\"\n",
    "\n",
    "df_manager = DataframeManager(url, dataset_name)\n",
    "\n",
    "df = df_manager.produce_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 5)\n",
      "(400, 5)\n",
      "(400, 5)\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = df_manager.split_df(RANDOM_SEED)\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode': 'utterance_3491',\n",
       " 'emotions': ['surprise', 'fear', 'surprise', 'sadness'],\n",
       " 'utterances': ['You-you\\x85you had sex with Ursula?!',\n",
       "  'Uh, a little bit. She-she-she walked in and I thought she was you and I kissed her and',\n",
       "  \"You didn't notice she was wearing different clothes?!\",\n",
       "  'Well I was just so excited to see you.'],\n",
       " 'triggers': [0, 0, 1, 0],\n",
       " 'emotions_id': [6, 5, 6, 0],\n",
       " '__index_level_0__': 3491}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_path = 'openlm-research/open_llama_3b_v2'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(model_path)\n",
    "# with init_empty_weights():\n",
    "#     model = AutoModel.from_config(config)\n",
    "# device_map = infer_auto_device_map(model)\n",
    "\n",
    "model = LlamaModel.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto', offload_folder=\"offload_base\", offload_state_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (LlamaModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'LlamaForCausalLM'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ: What is the largest animal?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marco\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marco\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1505\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1502\u001b[0m         synced_gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[1;32m-> 1505\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;66;03m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;66;03m# legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m     \u001b[38;5;66;03m# two conditions must be met\u001b[39;00m\n\u001b[0;32m   1511\u001b[0m     \u001b[38;5;66;03m# 1) the generation config must have been created from the model config (`_from_model_config` field);\u001b[39;00m\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# 2) the generation config must have seen no modification since its creation (the hash is the same).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marco\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1287\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate_compatible_classes:\n\u001b[0;32m   1286\u001b[0m     exception_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please use one of the following classes instead: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_compatible_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1287\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[1;31mTypeError\u001b[0m: The current model class (LlamaModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'LlamaForCausalLM'}"
     ]
    }
   ],
   "source": [
    "prompt = 'Q: What is the largest animal?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output = model.generate(input_ids)\n",
    "# output = class_model(input_ids)\n",
    "\n",
    "# generation_output = causal_model.generate(\n",
    "#     input_ids=input_ids, max_new_tokens=32\n",
    "# )\n",
    "# print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/accelerate-large-models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
