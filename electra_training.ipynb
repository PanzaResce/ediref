{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\marco\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from src.utils import *\n",
    "from src.models.models import Model_Phrase_Concatenation, Model_Phrase_Extraction, Model_concat_nopooling\n",
    "from src.models.baseline import Baseline\n",
    "from datasets import concatenate_datasets\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import DefaultDataCollator, AutoTokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "set_seeds(RANDOM_SEED)\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: c:\\Users\\marco\\OneDrive\\Immagini\\Documenti\\GitHub\\ediref\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "      <th>emotions_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>utterance_0</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise]</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[4, 4, 4, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>utterance_1</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[4, 4, 4, 4, 3, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utterance_2</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]</td>\n",
       "      <td>[4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>utterance_3</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 1, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utterance_4</td>\n",
       "      <td>[surprise, sadness, surprise, fear]</td>\n",
       "      <td>[But then who? The waitress I went out with la...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[3, 0, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>utterance_3995</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>utterance_3996</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>utterance_3997</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>utterance_3998</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>utterance_3999</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             episode                                           emotions  \\\n",
       "0        utterance_0     [neutral, neutral, neutral, neutral, surprise]   \n",
       "1        utterance_1  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "2        utterance_2  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "3        utterance_3  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "4        utterance_4                [surprise, sadness, surprise, fear]   \n",
       "...              ...                                                ...   \n",
       "3995  utterance_3995  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3996  utterance_3996  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3997  utterance_3997  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3998  utterance_3998  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3999  utterance_3999  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "\n",
       "                                             utterances  \\\n",
       "0     [also I was the point person on my company's t...   \n",
       "1     [also I was the point person on my company's t...   \n",
       "2     [also I was the point person on my company's t...   \n",
       "3     [also I was the point person on my company's t...   \n",
       "4     [But then who? The waitress I went out with la...   \n",
       "...                                                 ...   \n",
       "3995  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3996  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3997  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3998  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3999  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "\n",
       "                                               triggers  \\\n",
       "0                                       [0, 0, 0, 1, 0]   \n",
       "1                                 [0, 0, 0, 0, 0, 1, 0]   \n",
       "2                     [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]   \n",
       "3               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "4                                          [0, 0, 1, 0]   \n",
       "...                                                 ...   \n",
       "3995               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "3996         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "3997      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "3998   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "3999  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            emotions_id  \n",
       "0                                       [4, 4, 4, 4, 3]  \n",
       "1                                 [4, 4, 4, 4, 3, 4, 4]  \n",
       "2                     [4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 1]  \n",
       "3               [4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 1, 4, 3]  \n",
       "4                                          [3, 0, 3, 1]  \n",
       "...                                                 ...  \n",
       "3995               [4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2]  \n",
       "3996         [4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4]  \n",
       "3997      [4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4]  \n",
       "3998   [4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4, 3]  \n",
       "3999  [4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4, ...  \n",
       "\n",
       "[4000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://drive.google.com/uc?export=download&id=1wVNU2XvvhqjaGXZM-JLJwOt97gt4g9j2\"\n",
    "dataset_name = \"MELD_train_efr.json\"\n",
    "\n",
    "df_manager = DataframeManager(url, dataset_name)\n",
    "\n",
    "df = df_manager.produce_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = 'google/electra-base-discriminator'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "\n",
    "model_dir = \"./model_dir/\"+model_card.split(\"/\")[-1]+\"/\"\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokenized, val_data_tokenized, test_data_tokenized = df_manager.produce_dataset(tokenizer, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28062"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pos_weight(data, labels_column, class_weights=True, factor=1):\n",
    "    y = data[labels_column].numpy()\n",
    "    if class_weights:\n",
    "        return torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)).to(\"cuda\")\n",
    "    else:\n",
    "        return torch.tensor(compute_class_weight(class_weight=None, classes=np.unique(y), y=y)).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelTrainer(Trainer):\n",
    "    def __init__(self, pos_weight=None, **kwargs):\n",
    "        self.emotions_pos_weight, self.triggers_pos_weight = pos_weight\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        emotions_true = inputs[\"emotions_id_one_hot_encoding\"].to(\"cuda\")\n",
    "        triggers_true = inputs[\"triggers\"].unsqueeze(1).float().to(\"cuda\")\n",
    "\n",
    "        result = model(**inputs)\n",
    "        \n",
    "        emotion_logits = result['emotion_logits'].to(\"cuda\")\n",
    "        trigger_logits = result['trigger_logits'].to(\"cuda\")\n",
    "        \n",
    "        loss_fct_emotions = torch.nn.CrossEntropyLoss(weight=self.emotions_pos_weight).to(\"cuda\")        \n",
    "        loss_fct_triggers = torch.nn.BCEWithLogitsLoss(pos_weight=self.triggers_pos_weight[1]).to(\"cuda\")\n",
    "        \n",
    "        loss_emotions = loss_fct_emotions(emotion_logits, emotions_true.float())\n",
    "        loss_triggers = loss_fct_triggers(trigger_logits, triggers_true)\n",
    "\n",
    "\n",
    "        loss_emotions_wt = 0.5\n",
    "        loss_triggers_wt = 0.5\n",
    "\n",
    "        loss = loss_emotions_wt*loss_emotions + loss_triggers_wt*loss_triggers\n",
    "        return (loss, {'emotion_logits': emotion_logits, 'trigger_logits': trigger_logits}) if return_outputs else loss\n",
    "\n",
    "def get_trainer(model, train, val, model_dir, class_weights=True, batch_size=1, epochs=20):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_dir,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        metric_for_best_model='u_avg_f1',\n",
    "        report_to='none',\n",
    "        label_names=[\"emotions_id\", \"triggers\", \"dialogue_index\"],\n",
    "    )\n",
    "\n",
    "    full_dataset = concatenate_datasets([train_data_tokenized, val_data_tokenized, test_data_tokenized])\n",
    "    pos_weight = (init_pos_weight(full_dataset, df_manager.column_emotions_id, class_weights), init_pos_weight(full_dataset, df_manager.column_triggers, class_weights))\n",
    "\n",
    "    trainer = MultiLabelTrainer(\n",
    "        pos_weight=pos_weight,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda pred: compute_metrics(pred, len(df_manager.emotion2id.keys()))\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load a model\n",
    "# b = Model_Phrase_Extraction(len(df_manager.unique_emotions), tokenizer.sep_token_id)\n",
    "# model_path = model_dir + \"bert_extraction_51\"\n",
    "# b.load_state_dict(torch.load(model_path+\"/bert_extraction_51.pth\"))\n",
    "# tr = get_trainer(b, train_data_tokenized, val_data_tokenized, model_path, class_weights=True, batch_size=4, epochs=5)\n",
    "# tr.evaluate(val_data_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_extr_model(seed, freeze=False):\n",
    "    set_seeds(seed)\n",
    "    base_model = Model_Phrase_Extraction(len(df_manager.unique_emotions), tokenizer.sep_token_id, freeze)\n",
    "    model_path = model_dir+\"extraction_freeze_\"+str(seed) if freeze else model_dir+\"extraction_\"+str(seed)\n",
    "\n",
    "    trainer = get_trainer(base_model, train_data_tokenized, val_data_tokenized, model_path, class_weights=True, batch_size=4, epochs=1)\n",
    "\n",
    "    print(f'Training EXTRACTION MODEL with seed {seed}:')\n",
    "\n",
    "    trainer.train()\n",
    "    save_name = \"/extraction_freeze_\"+str(seed)+\".pth\" if freeze else \"/extraction_\"+str(seed)+\".pth\"\n",
    "    torch.save(base_model.state_dict(), model_path+save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [49, 666, 51, 77, 111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with seed 49\n",
    "train_extr_model(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with seed 666\n",
    "train_extr_model(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with seed 51\n",
    "train_extr_model(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with seed 77\n",
    "train_extr_model(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with seed 111\n",
    "train_extr_model(111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train freezed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with seed 49\n",
    "train_extr_model(49, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with seed 666\n",
    "train_extr_model(666, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with seed 51\n",
    "train_extr_model(51, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with seed 77\n",
    "train_extr_model(77, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with seed 111\n",
    "train_extr_model(111, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
