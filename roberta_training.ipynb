{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\marco\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from src.utils import *\n",
    "from src.models.models import Model_Phrase_Concatenation, Model_Phrase_Extraction, Model_concat_nopooling\n",
    "from src.models.baseline import Baseline\n",
    "from datasets import concatenate_datasets\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import DefaultDataCollator, AutoTokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "set_seeds(RANDOM_SEED)\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: c:\\Users\\marco\\OneDrive\\Immagini\\Documenti\\GitHub\\ediref\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "      <th>emotions_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>utterance_0</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise]</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[4, 4, 4, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>utterance_1</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[4, 4, 4, 4, 3, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utterance_2</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]</td>\n",
       "      <td>[4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>utterance_3</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 1, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utterance_4</td>\n",
       "      <td>[surprise, sadness, surprise, fear]</td>\n",
       "      <td>[But then who? The waitress I went out with la...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>[3, 0, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>utterance_3995</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>utterance_3996</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>utterance_3997</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>utterance_3998</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>utterance_3999</td>\n",
       "      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n",
       "      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             episode                                           emotions  \\\n",
       "0        utterance_0     [neutral, neutral, neutral, neutral, surprise]   \n",
       "1        utterance_1  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "2        utterance_2  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "3        utterance_3  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "4        utterance_4                [surprise, sadness, surprise, fear]   \n",
       "...              ...                                                ...   \n",
       "3995  utterance_3995  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3996  utterance_3996  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3997  utterance_3997  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3998  utterance_3998  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "3999  utterance_3999  [neutral, joy, neutral, neutral, surprise, dis...   \n",
       "\n",
       "                                             utterances  \\\n",
       "0     [also I was the point person on my company's t...   \n",
       "1     [also I was the point person on my company's t...   \n",
       "2     [also I was the point person on my company's t...   \n",
       "3     [also I was the point person on my company's t...   \n",
       "4     [But then who? The waitress I went out with la...   \n",
       "...                                                 ...   \n",
       "3995  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3996  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3997  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3998  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "3999  [Hey., Hey!, So how was Joan?, I broke up with...   \n",
       "\n",
       "                                               triggers  \\\n",
       "0                                       [0, 0, 0, 1, 0]   \n",
       "1                                 [0, 0, 0, 0, 0, 1, 0]   \n",
       "2                     [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]   \n",
       "3               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "4                                          [0, 0, 1, 0]   \n",
       "...                                                 ...   \n",
       "3995               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "3996         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "3997      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "3998   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "3999  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            emotions_id  \n",
       "0                                       [4, 4, 4, 4, 3]  \n",
       "1                                 [4, 4, 4, 4, 3, 4, 4]  \n",
       "2                     [4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 1]  \n",
       "3               [4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 1, 4, 3]  \n",
       "4                                          [3, 0, 3, 1]  \n",
       "...                                                 ...  \n",
       "3995               [4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2]  \n",
       "3996         [4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4]  \n",
       "3997      [4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4]  \n",
       "3998   [4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4, 3]  \n",
       "3999  [4, 6, 4, 4, 3, 2, 4, 2, 3, 4, 4, 2, 2, 4, 4, ...  \n",
       "\n",
       "[4000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://drive.google.com/uc?export=download&id=1wVNU2XvvhqjaGXZM-JLJwOt97gt4g9j2\"\n",
    "dataset_name = \"MELD_train_efr.json\"\n",
    "\n",
    "df_manager = DataframeManager(url, dataset_name)\n",
    "\n",
    "df = df_manager.produce_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265a7903b36447608c5fc5f2fc8eccd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\marco\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c4b0b9bc7941ebabab5eab5af4583d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8623438af571403d968f43a8f3b29920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf55f0ba7b14d188129afbb806424d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_card = 'bert-base-uncased'\n",
    "# model_card = 'roberta-base'\n",
    "model_card = 'google/electra-base-discriminator'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "\n",
    "# model_dir = \"./model_dir/\"+model_card+\"/\"\n",
    "model_dir = \"./model_dir/\"+model_card.split(\"/\")[-1]+\"/\"\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokenized, val_data_tokenized, test_data_tokenized = df_manager.produce_dataset(tokenizer, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pos_weight(data, labels_column, class_weights=True, factor=1):\n",
    "    y = data[labels_column].numpy()\n",
    "    if class_weights:\n",
    "        return torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)).to(\"cuda\")\n",
    "    else:\n",
    "        return torch.tensor(compute_class_weight(class_weight=None, classes=np.unique(y), y=y)).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelTrainer(Trainer):\n",
    "    def __init__(self, pos_weight=None, **kwargs):\n",
    "        self.emotions_pos_weight, self.triggers_pos_weight = pos_weight\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        emotions_true = inputs[\"emotions_id_one_hot_encoding\"].to(\"cuda\")\n",
    "        triggers_true = inputs[\"triggers\"].unsqueeze(1).float().to(\"cuda\")\n",
    "\n",
    "        result = model(**inputs)\n",
    "        \n",
    "        emotion_logits = result['emotion_logits'].to(\"cuda\")\n",
    "        trigger_logits = result['trigger_logits'].to(\"cuda\")\n",
    "        \n",
    "        loss_fct_emotions = torch.nn.CrossEntropyLoss(weight=self.emotions_pos_weight).to(\"cuda\")        \n",
    "        loss_fct_triggers = torch.nn.BCEWithLogitsLoss(pos_weight=self.triggers_pos_weight[1]).to(\"cuda\")\n",
    "\n",
    "        loss_emotions = loss_fct_emotions(emotion_logits, emotions_true.float())\n",
    "        loss_triggers = loss_fct_triggers(trigger_logits, triggers_true)\n",
    "\n",
    "        loss_emotions_wt = 0.5\n",
    "        loss_triggers_wt = 0.5\n",
    "\n",
    "        loss = loss_emotions_wt*loss_emotions + loss_triggers_wt*loss_triggers\n",
    "        return (loss, {'emotion_logits': emotion_logits, 'trigger_logits': trigger_logits}) if return_outputs else loss\n",
    "\n",
    "def get_trainer(model, train, val, model_dir, class_weights=True, batch_size=1, epochs=20):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_dir,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='u_avg_f1',\n",
    "        report_to='none',\n",
    "        label_names=[\"emotions_id\", \"triggers\", \"dialogue_index\"],\n",
    "    )\n",
    "\n",
    "    full_dataset = concatenate_datasets([train_data_tokenized, val_data_tokenized, test_data_tokenized])\n",
    "    pos_weight = (init_pos_weight(full_dataset, df_manager.column_emotions_id, class_weights), init_pos_weight(full_dataset, df_manager.column_triggers, class_weights))\n",
    "\n",
    "    trainer = MultiLabelTrainer(\n",
    "        pos_weight=pos_weight,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda pred: compute_metrics(pred, len(df_manager.emotion2id.keys())),\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\anaconda3\\lib\\site-packages\\datasets\\table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "c:\\Users\\marco\\anaconda3\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "c:\\Users\\marco\\anaconda3\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540945c1abb54e678f38980bea92e0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.023462644404346,\n",
       " 'eval_accuracy_emotions': 0.209,\n",
       " 'eval_accuracy_triggers': 0.8209,\n",
       " 'eval_u_avg_f1': 0.4422,\n",
       " 'eval_u_f1scores_emotions': 0.1308,\n",
       " 'eval_u_f1scores_triggers': 0.7537,\n",
       " 'eval_d_f1scores_emotions': 0.2149,\n",
       " 'eval_d_f1scores_triggers': 0.7526,\n",
       " 'eval_runtime': 13.4439,\n",
       " 'eval_samples_per_second': 4.984,\n",
       " 'eval_steps_per_second': 1.265}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to load a model\n",
    "b = Model_Phrase_Extraction(len(df_manager.unique_emotions), tokenizer.sep_token_id)\n",
    "model_path = model_dir + \"bert_extraction_51\"\n",
    "b.load_state_dict(torch.load(model_path+\"/bert_extraction_51.pth\"))\n",
    "tr = get_trainer(b, train_data_tokenized, val_data_tokenized, model_path, class_weights=True, batch_size=4, epochs=5)\n",
    "tr.evaluate(val_data_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_extr_model(seed):\n",
    "    set_seeds(seed)\n",
    "    base_model = Model_Phrase_Extraction(len(df_manager.unique_emotions), tokenizer.eos_token_id)\n",
    "    model_path = model_dir+\"extraction_\"+str(seed)\n",
    "\n",
    "    trainer = get_trainer(base_model, train_data_tokenized, val_data_tokenized, model_path, class_weights=True, batch_size=4, epochs=5)\n",
    "\n",
    "    print(f'Training EXTRACTION MODEL with seed {seed}:')\n",
    "\n",
    "    trainer.train()\n",
    "    torch.save(base_model.state_dict(), model_path+\"/extraction_\"+str(seed)+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [49, 666, 51, 77, 111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\anaconda3\\lib\\site-packages\\datasets\\table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "c:\\Users\\marco\\anaconda3\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training model with seed 111\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_extr_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m111\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mtrain_extr_model\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m      3\u001b[0m base_model \u001b[38;5;241m=\u001b[39m Model_Phrase_Extraction(\u001b[38;5;28mlen\u001b[39m(df_manager\u001b[38;5;241m.\u001b[39munique_emotions), tokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n\u001b[0;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m model_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextraction_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(seed)\n\u001b[1;32m----> 6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mget_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_tokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data_tokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining EXTRACTION MODEL with seed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mget_trainer\u001b[1;34m(model, train, val, model_dir, class_weights, batch_size, epochs)\u001b[0m\n\u001b[0;32m     28\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     29\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mmodel_dir,\n\u001b[0;32m     30\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     label_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotions_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtriggers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdialogue_index\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     45\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m concatenate_datasets([train_data_tokenized, val_data_tokenized, test_data_tokenized])\n\u001b[1;32m---> 46\u001b[0m pos_weight \u001b[38;5;241m=\u001b[39m (\u001b[43minit_pos_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_emotions_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m, init_pos_weight(full_dataset, df_manager\u001b[38;5;241m.\u001b[39mcolumn_triggers, class_weights))\n\u001b[0;32m     48\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MultiLabelTrainer(\n\u001b[0;32m     49\u001b[0m     pos_weight\u001b[38;5;241m=\u001b[39mpos_weight,\n\u001b[0;32m     50\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m pred: compute_metrics(pred, \u001b[38;5;28mlen\u001b[39m(df_manager\u001b[38;5;241m.\u001b[39memotion2id\u001b[38;5;241m.\u001b[39mkeys())),\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36minit_pos_weight\u001b[1;34m(data, labels_column, class_weights, factor)\u001b[0m\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m data[labels_column]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_weights:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(compute_class_weight(class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y), y\u001b[38;5;241m=\u001b[39my))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marco\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    297\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 298\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m    302\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# Training model with seed 111\n",
    "train_extr_model(111)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
